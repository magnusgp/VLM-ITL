project_name: "pascal-voc-segmentation"
run_name_prefix: "AL-b4-BD" # Run name will be appended with iteration info
output_dir_prefix: "./results/active_learning" # Output dir will be appended
seed: 42
log_with: "wandb"
debug: False # Set to True for debugging purposes

dataset:
  name: "nateraw/pascal-voc-2012"
  feature_extractor_name: "nvidia/segformer-b4-finetuned-ade-512-512"
  image_col: "image"
  mask_col: "mask"
  val_split_percentage: 0.1 # Use 10% of train as validation (fixed across iterations)
  test_split_percentage: 0.1 # Use 10% of train as test (fixed across iterations)
  load_from_cache_file: False # Set to True to use cached dataset

model:
  name: "nvidia/segformer-b4-finetuned-ade-512-512"
  num_labels: 21
  ignore_mismatched_sizes: True

training: # Params per Active Learning Iteration
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  num_train_epochs: 20 # Fewer epochs per iteration? Or same as baseline? Let's use fewer.
  learning_rate: 6e-5
  weight_decay: 0.01
  evaluation_strategy: "epoch"
  save_strategy: "epoch"
  save_total_limit: 1 # Only keep the best from the current iteration
  load_best_model_at_end: True
  metric_for_best_model: "mean_iou"
  logging_steps: 20
  remove_unused_columns: False
  fp16: True
  push_to_hub: False
  lr_scheduler_type: "polynomial"
  lr_scheduler_kwargs:
    power: 2.0
    lr_end: 0.0
  warmup_ratio: 0.1
  early_stopping_patience: 10 # Early stopping patience
  early_stopping_threshold: 0.001 # Early stopping threshold

active_learning:
  initial_percentage: 0.2 # Start with 20% of training data
  increment_percentage: 0.2 # Add 20% each iteration
  max_percentage: 1.0 # Go up to 100%
  sampling_strategy: "min_entropy" # Options: "random", "min_entropy", "max_entropy"
  iou_threshold: 0.6 # for entropy sampling